{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0qaZPCcOl3v9rrkP9NQLC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report"],"metadata":{"id":"CLFJf15dubW7","executionInfo":{"status":"ok","timestamp":1704209440127,"user_tz":-330,"elapsed":409,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('/content/HR_Data.csv')"],"metadata":{"id":"DUClqnTvveOs","executionInfo":{"status":"ok","timestamp":1704210167741,"user_tz":-330,"elapsed":394,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["print(data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zs7O7ifDz55n","executionInfo":{"status":"ok","timestamp":1704210189259,"user_tz":-330,"elapsed":399,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}},"outputId":"5340d2ad-5acb-4fbd-a03f-a93285ad2044"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n","0                0.38             0.53               2                   157   \n","1                0.80             0.86               5                   262   \n","2                0.11             0.88               7                   272   \n","3                0.72             0.87               5                   223   \n","4                0.37             0.52               2                   159   \n","\n","   time_spend_company  Work_accident  left  promotion_last_5years Department  \\\n","0                   3              0     1                      0      sales   \n","1                   6              0     1                      0      sales   \n","2                   4              0     1                      0      sales   \n","3                   5              0     1                      0      sales   \n","4                   3              0     1                      0      sales   \n","\n","   salary  \n","0     low  \n","1  medium  \n","2  medium  \n","3     low  \n","4     low  \n"]}]},{"cell_type":"code","source":["cat_vars = data.select_dtypes(include=['object']).columns\n","num_vars = data.select_dtypes(include=['int64', 'float64']).columns"],"metadata":{"id":"gYANYyTo1VSd","executionInfo":{"status":"ok","timestamp":1704210220708,"user_tz":-330,"elapsed":396,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["data = pd.get_dummies(data, columns=cat_vars, drop_first=True)"],"metadata":{"id":"w5Hzdm742FDT","executionInfo":{"status":"ok","timestamp":1704210245203,"user_tz":-330,"elapsed":534,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["X = data.drop('number_project', axis=1)\n","y = data['average_montly_hours']"],"metadata":{"id":"WdJ_SL1b2LLM","executionInfo":{"status":"ok","timestamp":1704210324696,"user_tz":-330,"elapsed":403,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"J_NoQjS_2edE","executionInfo":{"status":"ok","timestamp":1704210350357,"user_tz":-330,"elapsed":414,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"],"metadata":{"id":"J6YjTSCU2ldD","executionInfo":{"status":"ok","timestamp":1704210374788,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["model = LogisticRegression()\n","model.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"cWe0v27M2qlI","executionInfo":{"status":"ok","timestamp":1704210433618,"user_tz":-330,"elapsed":11879,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}},"outputId":"19590a61-8ef8-4afd-9948-27a0e05aba15"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["y_pred = model.predict(X_test)"],"metadata":{"id":"NJOJdNUr3MMg","executionInfo":{"status":"ok","timestamp":1704210536416,"user_tz":-330,"elapsed":415,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["print(confusion_matrix(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSHEGFaB3YY0","executionInfo":{"status":"ok","timestamp":1704210568946,"user_tz":-330,"elapsed":385,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}},"outputId":"618549ec-517f-4a29-948e-521bf6bd326f"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 3 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 3 0 0]\n"," [0 0 0 ... 0 1 0]\n"," [0 0 0 ... 0 0 3]]\n"]}]},{"cell_type":"code","source":["print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctbq2jKA3bC4","executionInfo":{"status":"ok","timestamp":1704210591413,"user_tz":-330,"elapsed":392,"user":{"displayName":"Harshitha Gowda","userId":"11733744624822367733"}},"outputId":"18e9c128-8135-44b8-a827-c5231d0deb84"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","          96       0.00      0.00      0.00         2\n","          97       0.00      0.00      0.00         1\n","          98       0.60      0.43      0.50         7\n","          99       0.00      0.00      0.00         1\n","         100       0.00      0.00      0.00         7\n","         101       0.00      0.00      0.00         5\n","         102       0.50      0.20      0.29         5\n","         103       0.00      0.00      0.00         5\n","         104       0.22      0.22      0.22         9\n","         105       0.00      0.00      0.00         2\n","         106       1.00      0.25      0.40         4\n","         107       0.00      0.00      0.00         2\n","         108       0.00      0.00      0.00         3\n","         109       0.00      0.00      0.00         2\n","         110       0.00      0.00      0.00         2\n","         111       0.20      0.20      0.20         5\n","         113       0.00      0.00      0.00         4\n","         114       0.00      0.00      0.00         4\n","         115       0.50      0.20      0.29         5\n","         116       0.00      0.00      0.00         3\n","         117       0.00      0.00      0.00         2\n","         118       0.00      0.00      0.00         2\n","         119       0.00      0.00      0.00         0\n","         120       0.00      0.00      0.00         2\n","         121       0.00      0.00      0.00         5\n","         122       0.00      0.00      0.00         4\n","         123       0.00      0.00      0.00         3\n","         124       0.00      0.00      0.00         2\n","         125       0.00      0.00      0.00         5\n","         126       0.00      0.00      0.00         1\n","         127       0.19      0.29      0.23        21\n","         128       0.50      0.12      0.20        16\n","         129       0.20      0.30      0.24        10\n","         130       0.14      0.10      0.12        10\n","         131       0.00      0.00      0.00         9\n","         132       0.19      0.12      0.15        24\n","         133       0.00      0.00      0.00        21\n","         134       0.13      0.07      0.09        28\n","         135       0.09      0.39      0.14        28\n","         136       0.08      0.04      0.05        28\n","         137       0.00      0.00      0.00        21\n","         138       0.00      0.00      0.00        12\n","         139       0.13      0.07      0.10        27\n","         140       0.17      0.27      0.21        26\n","         141       0.04      0.11      0.06        19\n","         142       0.00      0.00      0.00        21\n","         143       0.00      0.00      0.00        20\n","         144       0.00      0.00      0.00        16\n","         145       0.00      0.00      0.00        20\n","         146       0.03      0.05      0.04        22\n","         147       0.00      0.00      0.00        19\n","         148       0.00      0.00      0.00        28\n","         149       0.13      0.11      0.12        27\n","         150       0.07      0.25      0.11        16\n","         151       0.04      0.03      0.04        30\n","         152       0.00      0.00      0.00        27\n","         153       0.00      0.00      0.00        20\n","         154       0.08      0.08      0.08        26\n","         155       0.00      0.00      0.00        22\n","         156       0.09      0.48      0.15        27\n","         157       0.07      0.03      0.05        29\n","         158       0.00      0.00      0.00        31\n","         159       0.00      0.00      0.00        25\n","         160       0.13      0.10      0.12        29\n","         161       0.00      0.00      0.00        15\n","         162       0.00      0.00      0.00        19\n","         163       0.00      0.00      0.00        18\n","         164       0.00      0.00      0.00        19\n","         165       0.00      0.00      0.00        18\n","         166       0.00      0.00      0.00        15\n","         167       0.00      0.00      0.00        17\n","         168       0.00      0.00      0.00        18\n","         169       0.00      0.00      0.00        15\n","         170       0.03      0.17      0.05        12\n","         171       0.04      0.05      0.04        21\n","         172       0.00      0.00      0.00         9\n","         173       0.04      0.12      0.06        17\n","         174       0.25      0.18      0.21        17\n","         175       0.25      0.20      0.22        10\n","         176       0.00      0.00      0.00        20\n","         177       0.11      0.07      0.08        15\n","         178       0.00      0.00      0.00        20\n","         179       0.00      0.00      0.00        18\n","         180       0.00      0.00      0.00        13\n","         181       0.00      0.00      0.00        17\n","         182       0.11      0.05      0.07        21\n","         183       0.00      0.00      0.00        16\n","         184       0.04      0.07      0.05        15\n","         185       0.00      0.00      0.00        19\n","         186       0.00      0.00      0.00        17\n","         187       0.00      0.00      0.00        13\n","         188       0.00      0.00      0.00         9\n","         189       0.11      0.10      0.10        20\n","         190       0.05      0.20      0.08        10\n","         191       0.10      0.11      0.10        19\n","         192       0.04      0.10      0.06        20\n","         193       0.33      0.06      0.11        16\n","         194       0.00      0.00      0.00        19\n","         195       0.04      0.08      0.05        12\n","         196       0.05      0.06      0.05        18\n","         197       0.00      0.00      0.00        11\n","         198       0.00      0.00      0.00        19\n","         199       0.08      0.06      0.07        16\n","         200       0.00      0.00      0.00        12\n","         201       0.16      0.27      0.20        15\n","         202       0.08      0.06      0.07        17\n","         203       0.00      0.00      0.00        21\n","         204       0.00      0.00      0.00        12\n","         205       0.00      0.00      0.00        17\n","         206       0.12      0.12      0.12        16\n","         207       0.00      0.00      0.00        14\n","         208       0.05      0.07      0.06        14\n","         209       0.00      0.00      0.00         9\n","         210       0.00      0.00      0.00        12\n","         211       0.00      0.00      0.00        22\n","         212       0.00      0.00      0.00        11\n","         213       0.04      0.10      0.06        10\n","         214       0.15      0.23      0.18        13\n","         215       0.00      0.00      0.00        10\n","         216       0.00      0.00      0.00        14\n","         217       0.07      0.05      0.06        21\n","         218       0.10      0.09      0.10        11\n","         219       0.06      0.25      0.10        12\n","         220       0.00      0.00      0.00        12\n","         221       0.00      0.00      0.00        14\n","         222       0.00      0.00      0.00        19\n","         223       0.00      0.00      0.00        22\n","         224       0.13      0.27      0.17        33\n","         225       0.00      0.00      0.00        18\n","         226       0.00      0.00      0.00        18\n","         227       0.00      0.00      0.00        17\n","         228       0.12      0.12      0.12        16\n","         229       0.00      0.00      0.00        17\n","         230       0.00      0.00      0.00        15\n","         231       0.00      0.00      0.00        15\n","         232       0.00      0.00      0.00        17\n","         233       0.04      0.08      0.06        24\n","         234       0.14      0.15      0.14        20\n","         235       0.00      0.00      0.00        21\n","         236       0.00      0.00      0.00        17\n","         237       0.00      0.00      0.00        15\n","         238       0.07      0.11      0.09        19\n","         239       0.11      0.06      0.08        16\n","         240       0.00      0.00      0.00        19\n","         241       0.00      0.00      0.00        19\n","         242       0.00      0.00      0.00        19\n","         243       0.09      0.17      0.12        23\n","         244       0.04      0.04      0.04        24\n","         245       0.11      0.22      0.14        23\n","         246       0.06      0.13      0.09        15\n","         247       0.05      0.12      0.07        17\n","         248       0.00      0.00      0.00        19\n","         249       0.00      0.00      0.00        25\n","         250       0.11      0.12      0.12        24\n","         251       0.00      0.00      0.00        13\n","         252       0.29      0.13      0.18        15\n","         253       0.00      0.00      0.00        17\n","         254       0.06      0.07      0.06        15\n","         255       0.12      0.14      0.13        21\n","         256       0.00      0.00      0.00        15\n","         257       0.05      0.04      0.04        28\n","         258       0.00      0.00      0.00        24\n","         259       0.14      0.12      0.13        17\n","         260       0.08      0.08      0.08        26\n","         261       0.00      0.00      0.00        17\n","         262       0.33      0.10      0.15        20\n","         263       0.08      0.15      0.11        20\n","         264       0.03      0.04      0.04        25\n","         265       0.00      0.00      0.00        19\n","         266       0.00      0.00      0.00        28\n","         267       0.10      0.11      0.11        18\n","         268       0.07      0.11      0.09        18\n","         269       0.09      0.06      0.07        17\n","         270       0.14      0.04      0.07        23\n","         271       0.10      0.19      0.13        26\n","         272       0.00      0.00      0.00        23\n","         273       0.02      0.06      0.03        16\n","         274       0.06      0.06      0.06        17\n","         275       0.05      0.10      0.06        10\n","         276       0.00      0.00      0.00         6\n","         277       0.00      0.00      0.00         8\n","         278       0.12      0.25      0.16         8\n","         279       0.00      0.00      0.00         8\n","         280       0.17      0.09      0.12        11\n","         281       0.00      0.00      0.00         6\n","         282       0.00      0.00      0.00         6\n","         283       0.00      0.00      0.00         4\n","         284       0.00      0.00      0.00         7\n","         285       0.00      0.00      0.00         8\n","         286       0.00      0.00      0.00         8\n","         287       0.00      0.00      0.00         6\n","         288       0.00      0.00      0.00         2\n","         289       0.06      0.50      0.11         2\n","         290       0.00      0.00      0.00         1\n","         291       0.00      0.00      0.00         3\n","         292       0.17      0.33      0.22         3\n","         293       0.00      0.00      0.00         2\n","         294       0.00      0.00      0.00         5\n","         295       0.00      0.00      0.00         1\n","         296       0.00      0.00      0.00         7\n","         297       0.00      0.00      0.00         2\n","         298       0.00      0.00      0.00         4\n","         299       0.00      0.00      0.00         1\n","         300       0.00      0.00      0.00         3\n","         301       0.00      0.00      0.00         4\n","         303       0.00      0.00      0.00         1\n","         304       0.00      0.00      0.00         1\n","         305       0.00      0.00      0.00         1\n","         306       0.00      0.00      0.00         3\n","         307       0.00      0.00      0.00         5\n","         308       0.27      1.00      0.43         3\n","         309       0.20      0.50      0.29         2\n","         310       0.38      1.00      0.55         3\n","\n","    accuracy                           0.07      3000\n","   macro avg       0.06      0.07      0.05      3000\n","weighted avg       0.06      0.07      0.05      3000\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}]}